{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Notebook completo:"
      ],
      "metadata": {
        "id": "VJeK7cpG3VAW"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZUKyS2HI3QiV"
      },
      "outputs": [],
      "source": [
        "# Instalación de las librerías necesarias (solo ejecuta esta celda una vez)\n",
        "!pip install langchain langchain_ollama chromadb sentence-transformers langchain_huggingface langchain_chroma beautifulsoup4\n",
        "\n",
        "# Importación de las librerías necesarias\n",
        "from langchain_huggingface import HuggingFaceEmbeddings\n",
        "from langchain_chroma import Chroma\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain_ollama.chat_models import ChatOllama\n",
        "from langchain_core.runnables import RunnableLambda, RunnablePassthrough\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 1: Cargar datos desde una página web"
      ],
      "metadata": {
        "id": "gvPbEcqf3YGm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Cargar los datos desde una página WEB (Wikipedia)\n",
        "url = 'https://en.wikipedia.org/wiki/Football'\n",
        "response = requests.get(url)\n",
        "\n",
        "# Comprobamos si la página se cargó correctamente\n",
        "if response.status_code == 200:\n",
        "    print(\"Página cargada correctamente\")\n",
        "\n",
        "    # Usamos BeautifulSoup para analizar el contenido HTML de la página\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Extraemos solo los párrafos (etiquetas <p>) de la página\n",
        "    paragraphs = soup.find_all('p')\n",
        "\n",
        "    # Filtramos el contenido relevante de los párrafos\n",
        "    clean_text = ''\n",
        "    for paragraph in paragraphs:\n",
        "        # Extraemos el texto de cada párrafo, eliminamos los saltos de línea y los espacios extra\n",
        "        clean_text += paragraph.get_text(separator=' ', strip=True) + ' '\n",
        "\n",
        "    # Limpieza adicional para el texto\n",
        "    # Elimina las referencias de texto como [1], [2], etc.\n",
        "    clean_text = re.sub(r'\\[.*?\\]', '', clean_text)\n",
        "\n",
        "    # Elimina los espacios extras\n",
        "    clean_text = re.sub(r'\\s+', ' ', clean_text)\n",
        "\n",
        "    # Elimina posibles saltos de línea innecesarios\n",
        "    clean_text = clean_text.replace(\"\\n\", \" \")\n",
        "\n",
        "    # Mostramos una parte del texto limpio (primeros 1000 caracteres)\n",
        "    print(\"Texto de la página cargado:\")\n",
        "    print(clean_text[:1000])  # Muestra los primeros 1000 caracteres del texto limpio\n",
        "else:\n",
        "    raise Exception(f\"Error al cargar la página: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "T2mWXHSR3aJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del código:\n",
        "\n",
        "**1. Instalación de librerías**: Instalamos las librerías necesarias para el procesamiento de texto, la creación de embeddings (vectorización), y la interacción con modelos de chat. Aquí usamos beautifulsoup4 para extraer el texto de una página web.\n",
        "\n",
        "**2. Cargar datos desde Wikipedia**: Cargamos el contenido de la página de Wikipedia sobre el fútbol y usamos BeautifulSoup para extraer solo los párrafos (p). El texto extraído se limpia para eliminar referencias y saltos de línea innecesarios, dejándonos con solo el texto relevante."
      ],
      "metadata": {
        "id": "-A-rWpyn3cX-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 2: Preparar el modelo de embeddings y la base de datos"
      ],
      "metadata": {
        "id": "DeRb28NV3eRu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Seleccionar el modelo de embeddings (vectorización del texto)\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"sentence-transformers/all-mpnet-base-v2\")\n",
        "\n",
        "# Crear una base de datos de Chroma para almacenar y buscar los embeddings\n",
        "vector_store = Chroma(\n",
        "    collection_name=\"rag1\",  # Nombre de la colección de datos\n",
        "    embedding_function=embeddings,  # Función de embeddings para convertir el texto en vectores\n",
        "    persist_directory=\"./datasets\",  # Directorio donde se guardará la base de datos\n",
        ")\n",
        "\n",
        "# Configurar el \"retriever\" para acceder a la base de datos\n",
        "retriever = vector_store.as_retriever()\n",
        "\n",
        "print(\"Base de datos y retriever configurados correctamente.\")\n"
      ],
      "metadata": {
        "id": "qPx8Skk33d0O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del código:\n",
        "\n",
        "**1. Embeddings**: Usamos el modelo de embeddings de sentence-transformers/all-mpnet-base-v2 para convertir el texto en vectores de alta dimensión que el modelo de IA pueda procesar.\n",
        "\n",
        "**2. Base de datos Chroma**: Usamos Chroma como base de datos para almacenar estos embeddings. Esta base de datos nos permitirá realizar búsquedas de texto basado en el contenido vectorizado.\n",
        "\n",
        "**3. Retriever**: El retriever es la herramienta que nos permitirá buscar el contenido dentro de la base de datos Chroma, basándonos en el texto de entrada."
      ],
      "metadata": {
        "id": "duLtyLMR3ijt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 3: Configurar el modelo de IA y la cadena RAG"
      ],
      "metadata": {
        "id": "lL9UuJdI4PLt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Plantilla de conversación (prompt) que se usará para generar respuestas\n",
        "conversation_template = \"\"\"Please answer the following question based only on the given context:\n",
        "{context}\n",
        "\n",
        "Question: {question}\n",
        "\"\"\"\n",
        "\n",
        "# Crear el prompt a partir de la plantilla\n",
        "prompt_template = ChatPromptTemplate.from_template(conversation_template)\n",
        "\n",
        "# Configurar el modelo local de Ollama (cambiar a tu modelo disponible)\n",
        "local_llm = \"tinyllama\"  # Asegúrate de que este modelo esté disponible en tu entorno\n",
        "local_model = ChatOllama(model=local_llm)\n",
        "\n",
        "# Crear la cadena de procesamiento (RAG Chain)\n",
        "qa_chain = (\n",
        "    {\"context\": retriever, \"question\": RunnablePassthrough()}  # Retriver obtiene el contexto\n",
        "    | prompt_template  # El template crea el prompt a partir del contexto y la pregunta\n",
        "    | local_model  # El modelo genera la respuesta\n",
        "    | StrOutputParser()  # Se procesa la salida generada por el modelo\n",
        ")\n",
        "\n",
        "print(\"Cadena RAG configurada correctamente.\")\n"
      ],
      "metadata": {
        "id": "v881gGv-3kWm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del código:\n",
        "\n",
        "**1. Plantilla de conversación**: Creamos una plantilla de conversación que pide una respuesta basada solo en el contexto proporcionado.\n",
        "\n",
        "**2. Modelo local**: Usamos ChatOllama con el modelo tinyllama (puedes cambiarlo si tienes otros modelos disponibles).\n",
        "\n",
        "**3. Cadena de procesamiento RAG**: La cadena de procesamiento recibe el contexto de Chroma, lo convierte en un prompt, y luego pasa el prompt al modelo para generar una respuesta."
      ],
      "metadata": {
        "id": "4lt2TA3C4TLl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Paso 4: Ejecutar una pregunta y obtener la respuesta"
      ],
      "metadata": {
        "id": "R8Q1cBFE4dzU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Ejecutamos una pregunta para probar el sistema\n",
        "result = qa_chain.invoke(\"What is football?\")\n",
        "print(\"Respuesta de la IA:\")\n",
        "print(result)"
      ],
      "metadata": {
        "id": "UQMoPHPT4SfM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explicación del código:\n",
        "\n",
        "**1. Invocar la cadena RAG**: Hacemos una consulta (en este caso, \"What is football?\") y ejecutamos el sistema.\n",
        "\n",
        "**2. Obtener la respuesta**: El modelo generará una respuesta basada en el contexto que le proporcionamos de la página web de Wikipedia."
      ],
      "metadata": {
        "id": "mVsBX-eJ4hVk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Conclusión:**\n",
        "\n",
        "Este notebook te permite extraer texto de una página web (en este caso, Wikipedia sobre el fútbol), procesarlo, y luego usar ese contenido como contexto para un modelo de IA que genera respuestas basadas en el contexto."
      ],
      "metadata": {
        "id": "v1boZyik4mn0"
      }
    }
  ]
}